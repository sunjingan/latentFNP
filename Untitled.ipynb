{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b59c26-519b-4d38-82be-a0bb9948ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/sunjingan/anaconda3/envs/FNP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/petrelfs/sunjingan/anaconda3/envs/FNP/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from dict_recursive_update import recursive_update\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from einops import rearrange\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "cwd = os.getcwd()\n",
    "try:\n",
    "    from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
    "    ATTENTION_MODE = 'flash'\n",
    "except:\n",
    "    ATTENTION_MODE = 'math'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f966ad4-23e1-4080-b3a7-da67a6e55639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., bias=True):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads=8, qkv_bias=False, window_size=None, rel_pos_spatial=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.rel_pos_spatial = rel_pos_spatial\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.window_size = window_size\n",
    "        if COMPAT:\n",
    "            if COMPAT == 2:\n",
    "                self.rel_pos_h = nn.Parameter(torch.zeros(2 * window_size[0] - 1, head_dim))\n",
    "                self.rel_pos_w = nn.Parameter(torch.zeros(2 * window_size[1] - 1, head_dim))\n",
    "            else:\n",
    "                q_size = window_size[0]\n",
    "                kv_size = q_size\n",
    "                rel_sp_dim = 2 * q_size - 1\n",
    "                self.rel_pos_h = nn.Parameter(torch.zeros(rel_sp_dim, head_dim))\n",
    "                self.rel_pos_w = nn.Parameter(torch.zeros(rel_sp_dim, head_dim))\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        \n",
    "        if ATTENTION_MODE == 'math':\n",
    "            qkv = rearrange(qkv, 'B N (K H D) -> K B H N D', K=3, H=self.num_heads)\n",
    "            q, k, v = qkv.unbind(0) # make torchscript happy (cannot use tensor as tuple)   --> (batchsize, heads, len, head_dim)\n",
    "            attn = ((q * self.scale) @ k.transpose(-2, -1))\n",
    "            attn = torch.softmax(attn, dim=-1)\n",
    "            x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        # =====================================\n",
    "        elif ATTENTION_MODE == 'flash':\n",
    "            qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "            data_type = qkv.dtype\n",
    "            qkv=qkv.to(torch.float16) \n",
    "            x = flash_attn_qkvpacked_func(qkv, dropout_p=0.0, softmax_scale=self.scale, causal=False).reshape(B, N, C)    \n",
    "            x=x.to(data_type)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))\n",
    "    x = windows.view(B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def calc_rel_pos_spatial(\n",
    "        attn,\n",
    "        q,\n",
    "        q_shape,\n",
    "        k_shape,\n",
    "        rel_pos_h,\n",
    "        rel_pos_w,\n",
    "):\n",
    "    \"\"\"\n",
    "    Spatial Relative Positional Embeddings.\n",
    "\n",
    "    Source: https://github.com/facebookresearch/mvit/\n",
    "    \"\"\"\n",
    "    sp_idx = 0\n",
    "    q_h, q_w = q_shape\n",
    "    k_h, k_w = k_shape\n",
    "\n",
    "    # Scale up rel pos if shapes for q and k are different.\n",
    "    q_h_ratio = max(k_h / q_h, 1.0)\n",
    "    k_h_ratio = max(q_h / k_h, 1.0)\n",
    "    dist_h = (torch.arange(q_h)[:, None] * q_h_ratio - torch.arange(k_h)[None, :] * k_h_ratio)\n",
    "    dist_h += (k_h - 1) * k_h_ratio\n",
    "    q_w_ratio = max(k_w / q_w, 1.0)\n",
    "    k_w_ratio = max(q_w / k_w, 1.0)\n",
    "    dist_w = (torch.arange(q_w)[:, None] * q_w_ratio - torch.arange(k_w)[None, :] * k_w_ratio)\n",
    "    dist_w += (k_w - 1) * k_w_ratio\n",
    "\n",
    "    Rh = rel_pos_h[dist_h.long()]\n",
    "    Rw = rel_pos_w[dist_w.long()]\n",
    "\n",
    "    B, n_head, q_N, dim = q.shape\n",
    "\n",
    "    r_q = q[:, :, sp_idx:].reshape(B, n_head, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"byhwc,hkc->byhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"byhwc,wkc->byhwk\", r_q, Rw)\n",
    "\n",
    "    attn[:, :, sp_idx:, sp_idx:] = (\n",
    "            attn[:, :, sp_idx:, sp_idx:].view(B, -1, q_h, q_w, k_h, k_w)\n",
    "            + rel_h[:, :, :, :, :, None]\n",
    "            + rel_w[:, :, :, :, None, :]\n",
    "    ).view(B, -1, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    return attn\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, rel_pos_spatial=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.rel_pos_spatial=rel_pos_spatial\n",
    "\n",
    "        if COMPAT:\n",
    "            q_size = window_size[0]\n",
    "            kv_size = window_size[1]\n",
    "            rel_sp_dim = 2 * q_size - 1\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(rel_sp_dim, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(rel_sp_dim, head_dim))\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "\n",
    "        x = x.reshape(B_, H, W, C)\n",
    "\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]\n",
    "        pad_b = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]\n",
    "\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        x = window_partition(x, self.window_size)  # num_Windows*B, window_size, window_size, C\n",
    "        x = x.view(-1, self.window_size[1] * self.window_size[0], C)  # num_Windows*B, window_size*window_size, C\n",
    "\n",
    "        B_w = x.shape[0]\n",
    "        N_w = x.shape[1]\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B_w, N_w, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)   --> (batchsize, heads, len, head_dim)\n",
    "        attn = ((q * self.scale) @ k.transpose(-2, -1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_w, N_w, C)\n",
    "            \n",
    "        x = self.proj(x)\n",
    "\n",
    "        x = x.view(-1, self.window_size[1], self.window_size[0], C)\n",
    "        x = window_reverse(x, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B_, H * W, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, window=False, rel_pos_spatial=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        \n",
    "        if not window:\n",
    "            print(\"MHA\")\n",
    "            self.attn = Attention(\n",
    "                dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "                window_size=window_size, rel_pos_spatial=rel_pos_spatial)\n",
    "        else:\n",
    "        \n",
    "            self.attn = WindowAttention(\n",
    "                dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "                window_size=window_size, rel_pos_spatial=rel_pos_spatial,\n",
    "            )\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
    "       \n",
    "    def forward(self, x, H, W, mask=None):\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16,patch_stride=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patch_stride = to_2tuple(patch_stride)\n",
    "        self.img_size = img_size\n",
    "        self.patch_shape = (img_size[0] // patch_stride[0], img_size[1] // patch_stride[1])  # could be dynamic\n",
    "        self.num_patches = self.patch_shape[0] * self.patch_shape[1]  # could be dynamic\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_stride)\n",
    "\n",
    "    def forward(self, x, mask=None, **kwargs):\n",
    "        \n",
    "        x = self.proj(x) \n",
    "        Hp, Wp = x.shape[2], x.shape[3]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.interpolate(mask[None].float(), size=(Hp, Wp)).to(torch.bool)[0]\n",
    "\n",
    "        return x, (Hp, Wp), mask\n",
    "\n",
    "\n",
    "class Norm2d(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.ln(x)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5e7015-2bb4-4f60-96e9-b686501f6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViT_Encoder(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 img_size=224,\n",
    "                 patch_size=16, \n",
    "                 patch_stride=16, \n",
    "                 in_chans=69, \n",
    "                 out_chans=227,\n",
    "                 z_dim=None,\n",
    "                 embed_dim=768, \n",
    "                 depth=12,\n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False, \n",
    "                 window_size=(14,14),\n",
    "                 drop_path_rate=0.,\n",
    "                 norm_layer=None,\n",
    "                 window=True,\n",
    "                 use_abs_pos_emb=False,\n",
    "                 interval=3, \n",
    "                 bn_group=None, \n",
    "                 test_pos_mode='simple_interpolate',\n",
    "                 learnable_pos=False, \n",
    "                 rel_pos_spatial=False, \n",
    "                 lms_checkpoint_train=False, \n",
    "                 pad_attn_mask=False, \n",
    "                 freeze_iters=0,\n",
    "                 act_layer='GELU', \n",
    "                 pre_ln=False, \n",
    "                 mask_input=False, \n",
    "                 ending_norm=True,\n",
    "                 round_padding=False, \n",
    "                 compat=False):\n",
    "        super().__init__()\n",
    "        self.pad_attn_mask = pad_attn_mask  # only effective for detection task input w/ NestedTensor wrapping\n",
    "        self.lms_checkpoint_train = lms_checkpoint_train\n",
    "        self.freeze_iters = freeze_iters\n",
    "        self.mask_input = mask_input\n",
    "        self.ending_norm = ending_norm\n",
    "        self.round_padding = round_padding\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.depth = depth\n",
    "        self.num_heads =num_heads\n",
    "        self.Hp, self.Wp = 0, 0\n",
    "        self.ori_Hp, self.ori_Hw = img_size[0] // patch_size[0], \\\n",
    "                                   img_size[1] // patch_size[1]\n",
    "\n",
    "        global COMPAT\n",
    "        COMPAT = compat\n",
    "\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.z_dim =z_dim\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, patch_stride= patch_stride,\n",
    "            in_chans=in_chans, embed_dim=embed_dim)\n",
    "\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        if use_abs_pos_emb:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=learnable_pos)\n",
    "            pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], self.patch_embed.patch_shape, cls_token=False)\n",
    "\n",
    "            self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(0, depth//2):\n",
    "            which_win = min(i%interval, len(window_size)-1)\n",
    "            block = Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                window_size=window_size[which_win] if ((i + 1) % interval != 0) else self.patch_embed.patch_shape,\n",
    "                window=((i + 1) % interval != 0) if window else False,\n",
    "                rel_pos_spatial=rel_pos_spatial,\n",
    "                act_layer=QuickGELU if act_layer == 'QuickGELU' else nn.GELU\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "            if i == depth//2-1:\n",
    "                block = Block(\n",
    "                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                    drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                    window_size=window_size[which_win] if ((i + 1) % interval != 0) else self.patch_embed.patch_shape,\n",
    "                    window=((i + 1) % interval != 0) if window else False,\n",
    "                    rel_pos_spatial=rel_pos_spatial,\n",
    "                    act_layer=QuickGELU if act_layer == 'QuickGELU' else nn.GELU\n",
    "                )\n",
    "                self.blocks.append(block)\n",
    "\n",
    "        self.ln_pre = norm_layer(embed_dim) if pre_ln else nn.Identity()  # for clip model only\n",
    "\n",
    "        if self.z_dim is not None:\n",
    "            self.quan_mlp = Mlp(in_features=2*embed_dim,\n",
    "                                hidden_features=2*int(np.sqrt(embed_dim//z_dim))*z_dim,\n",
    "                                out_features=2*z_dim )\n",
    "\n",
    "        ### duplicated init, only affects network weights and has no effect given pretrain\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        ###\n",
    "        self.test_pos_mode = test_pos_mode\n",
    "        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if self.mask_input else None\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def embedding_forward(self, x, mask=None, **kwargs):\n",
    "        x, (self.Hp, self.Wp), mask = self.patch_embed(x, mask)\n",
    "        print(\"after patch embed\",x.shape,self.Hp, self.Wp)\n",
    "        print(\"pos_embed:\",self.pos_embed.shape)\n",
    "        x = x + self.pos_embed      #get_abs_pos(pos_embed, False, (self.ori_Hp, self.ori_Hw), patch_shape)\n",
    "        return x\n",
    "    \n",
    "    def encoder_forward(self, x):\n",
    "        # x = self.ln_pre(x)  # effective for clip model only, otherwise nn.Identity\n",
    "        for i in range(len(self.blocks)-1):\n",
    "            \n",
    "\n",
    "            if i == len(self.blocks)-2:\n",
    "                mean = self.blocks[i](x, self.Hp, self.Wp)\n",
    "                logvar = self.blocks[i+1](x, self.Hp, self.Wp)\n",
    "                x = torch.cat([mean,logvar], 2)\n",
    "\n",
    "                return x  #  we here control the last 2 blocks to genertate 2*dimensions' data.\n",
    "            else:\n",
    "                x = self.blocks[i](x, self.Hp, self.Wp)\n",
    "            print(\"after blocks\",i,x.shape,self.Hp, self.Wp)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_var, **kwargs):\n",
    "        x = self.embedding_forward(input_var, **kwargs)\n",
    "        print(\"after embedding\",x.shape)\n",
    "        x = self.encoder_forward(x)\n",
    "\n",
    "        if self.z_dim is not None:\n",
    "            x = self.quan_mlp(x)\n",
    "        B ,N ,C = x.shape\n",
    "        x = x.reshape(B, self.Hp, self.Wp, C).permute(0,3,1,2)\n",
    "\n",
    "        return x\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_size = to_2tuple(grid_size)\n",
    "    grid_h = np.arange(grid_size[0], dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size[1], dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    grid = grid.reshape([2, 1, grid_size[0], grid_size[1]])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "def get_abs_pos(abs_pos, has_cls_token, ori_hw, hw):\n",
    "    \"\"\"\n",
    "    Calculate absolute positional embeddings. If needed, resize embeddings and remove cls_token\n",
    "        dimension for the original embeddings.\n",
    "    Args:\n",
    "        abs_pos (Tensor): absolute positional embeddings with (1, num_position, C).\n",
    "        has_cls_token (bool): If true, has 1 embedding in abs_pos for cls token.\n",
    "        hw (Tuple): size of input image tokens.\n",
    "    Returns:\n",
    "        Absolute positional embeddings after processing with shape (1, H, W, C)\n",
    "    \"\"\"\n",
    "    embed_num, _, emde_dim = abs_pos.size()\n",
    "    h, w = hw\n",
    "    if has_cls_token:\n",
    "     abs_pos = abs_pos[:, 1:]\n",
    "    xy_num = abs_pos.shape[1]\n",
    "    size = int(math.sqrt(xy_num))\n",
    "\n",
    "    ori_hp, ori_hw = ori_hw\n",
    "\n",
    "    assert ori_hp, ori_hw == xy_num\n",
    "\n",
    "    if ori_hp != h or ori_hw != w:\n",
    "        new_abs_pos = F.interpolate(\n",
    "            abs_pos.reshape(embed_num, ori_hp, ori_hw, -1).permute(0, 3, 1, 2),\n",
    "            size=(h, w),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        return new_abs_pos.permute(0, 2, 3, 1).reshape(embed_num, h*w, -1)\n",
    "    else:\n",
    "        return abs_pos.reshape(embed_num, h*w, -1)\n",
    "\n",
    "class ViT_Decoder(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                img_size=224, \n",
    "                patch_size=16, \n",
    "                patch_stride=16, \n",
    "                in_chans=3, \n",
    "                out_chans=227, \n",
    "                z_dim=None,\n",
    "                embed_dim=768, \n",
    "                depth=12,\n",
    "                num_heads=12, \n",
    "                mlp_ratio=4., \n",
    "                qkv_bias=False, \n",
    "                window_size=(14,14), \n",
    "                drop_path_rate=0., \n",
    "                norm_layer=None, \n",
    "                window=True,\n",
    "                use_abs_pos_emb=False, \n",
    "                interval=3, \n",
    "                bn_group=None, \n",
    "                test_pos_mode='simple_interpolate',\n",
    "                learnable_pos=False, \n",
    "                rel_pos_spatial=False, \n",
    "                lms_checkpoint_train=False,\n",
    "                pad_attn_mask=False, \n",
    "                freeze_iters=0,\n",
    "                act_layer='GELU', \n",
    "                pre_ln=False, \n",
    "                mask_input=False, \n",
    "                ending_norm=True,\n",
    "                round_padding=False, \n",
    "                compat=False):\n",
    "        super().__init__()\n",
    "        self.pad_attn_mask = pad_attn_mask  # only effective for detection task input w/ NestedTensor wrapping\n",
    "        self.lms_checkpoint_train = lms_checkpoint_train\n",
    "        self.freeze_iters = freeze_iters\n",
    "        self.mask_input = mask_input\n",
    "        self.ending_norm = ending_norm\n",
    "        self.round_padding = round_padding\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.depth = depth\n",
    "        self.num_heads =num_heads\n",
    "        self.Hp, self.Wp = img_size[0] // patch_stride[0], \\\n",
    "                                   img_size[1] // patch_stride[1]\n",
    "        global COMPAT\n",
    "        COMPAT = compat\n",
    "        self.patch_shape = (self.Hp, self.Wp)\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.z_dim=z_dim\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        if z_dim is not None:\n",
    "            self.post_quan_mlp = Mlp(in_features=z_dim,\n",
    "                                hidden_features=int(np.sqrt(embed_dim//z_dim))*z_dim,\n",
    "                                out_features=embed_dim)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(depth//2, depth):\n",
    "            which_win = min(i%interval, len(window_size)-1)\n",
    "            block = Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                window_size=window_size[which_win] if ((i + 1) % interval != 0) else self.patch_shape,\n",
    "                window=((i + 1) % interval != 0) if window else False,\n",
    "                rel_pos_spatial=rel_pos_spatial,\n",
    "                act_layer=QuickGELU if act_layer == 'QuickGELU' else nn.GELU\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.ln_pre = norm_layer(embed_dim) if pre_ln else nn.Identity()  # for clip model only\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        if self.img_size==(721, 1440):\n",
    "            self.final = nn.ConvTranspose2d(in_channels=embed_dim, out_channels=out_chans,\n",
    "                                            kernel_size=patch_size, stride=patch_stride, bias=False)\n",
    "        else:\n",
    "            self.final = nn.Linear(embed_dim, out_chans*patch_size[-1]*patch_size[-2], bias=False)\n",
    "\n",
    "        ### duplicated init, only affects network weights and has no effect given pretrain\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def decoder_forward(self, x):\n",
    "        # x = self.ln_pre(x)  # effective for clip model only, otherwise nn.Identity\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, self.Hp, self.Wp)\n",
    "            print(\"after block\",i,x.shape)\n",
    "\n",
    "        if self.ending_norm:\n",
    "\n",
    "            x = self.norm(x)  # b h*w c\n",
    "        return x\n",
    "    def up_forward(self, x):\n",
    "        x = x.view(x.size(0), self.Hp, self.Wp,-1)\n",
    "        if self.img_size==(721, 1440):\n",
    "            res = self.final(x.permute(0, 3, 1, 2))\n",
    "            return res\n",
    "        else:\n",
    "            x = self.final(x)\n",
    "            res = rearrange(\n",
    "                x,\n",
    "                \"b h w (p1 p2 c_out) -> b c_out (h p1) (w p2)\",\n",
    "                p1=self.patch_size[-2],\n",
    "                p2=self.patch_size[-1],\n",
    "                h=self.img_size[0] // self.patch_size[-2],\n",
    "                w=self.img_size[1] // self.patch_size[-1],\n",
    "            )\n",
    "            return res\n",
    "\n",
    "    def forward(self, feat, **kwargs):\n",
    "        B, C, H,W = feat.shape\n",
    "        print(\"after encoder\",feat.shape)\n",
    "        x = feat.reshape(B, C,-1).permute(0,2,1)\n",
    "\n",
    "        if self.z_dim is not None:\n",
    "            x = self.post_quan_mlp(x)\n",
    "            print(\"after post mlp\",x.shape)\n",
    "\n",
    "        out = self.decoder_forward(x)\n",
    "        print(\"after decoder\",out.shape)\n",
    "\n",
    "        out = self.up_forward(out)\n",
    "        print(\"after up\",out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f8a758-2749-4a80-a1f1-d2c694771395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(arch='vit_base', patch_size=(16,16), patch_stride=None, in_chans=227, out_chans=227,\n",
    "                 pretrained_model=None, finetune_model=None,kwargs=None):\n",
    "\n",
    "        if patch_stride is None:\n",
    "            patch_stride =patch_size\n",
    "\n",
    "        base_default_dict =dict(\n",
    "            drop_path_rate=0, use_abs_pos_emb=True,  # as in table 11\n",
    "            patch_size=patch_size, patch_stride=patch_stride, in_chans=in_chans, out_chans=out_chans, embed_dim=768, depth=12,\n",
    "            num_heads=12, mlp_ratio=4, qkv_bias=True, \n",
    "\n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            # learnable_pos= True,\n",
    "        )\n",
    "\n",
    "        large_default_dict =dict(\n",
    "            drop_path_rate=0, use_abs_pos_emb=True,  # as in table 11\n",
    "            patch_size=patch_size,patch_stride=patch_stride,in_chans=in_chans, out_chans=out_chans, embed_dim=1024, depth=24,\n",
    "            num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            # learnable_pos= True,\n",
    "        )\n",
    "\n",
    "\n",
    "        huge_default_dict =dict(\n",
    "            drop_path_rate=0, use_abs_pos_emb=True,  # as in table 11\n",
    "            patch_size=patch_size,patch_stride=patch_stride,in_chans=in_chans, out_chans=out_chans, embed_dim=2048, depth=24,\n",
    "            num_heads=16, mlp_ratio=4, qkv_bias=True, \n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            # learnable_pos= True,\n",
    "        )\n",
    "\n",
    "        if arch == \"vit_base\":\n",
    "            recursive_update(base_default_dict, kwargs)\n",
    "            encoder = ViT_Encoder(**base_default_dict)\n",
    "\n",
    "        elif arch == \"vit_large\":\n",
    "\n",
    "            recursive_update(large_default_dict, kwargs)\n",
    "            encoder = ViT_Encoder(**large_default_dict)\n",
    "\n",
    "        elif arch == \"vit_huge\":\n",
    "            recursive_update(huge_default_dict, kwargs)\n",
    "            encoder = ViT_Encoder(**huge_default_dict)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Architecture undefined!\")\n",
    "\n",
    "\n",
    "        return encoder\n",
    "\n",
    "def Decoder(arch='vit_base', patch_size=(16,16),patch_stride=None, in_chans=227, out_chans=227,\n",
    "            pretrained_model=None, finetune_model=None, kwargs=None):\n",
    "\n",
    "    if patch_stride is None:\n",
    "        patch_stride =patch_size\n",
    "\n",
    "    base_default_dict =dict(\n",
    "        drop_path_rate=0, use_abs_pos_emb=True,  # as in table 11\n",
    "        patch_size=patch_size, patch_stride=patch_stride, in_chans=in_chans, out_chans=out_chans, embed_dim=768, depth=12,\n",
    "        num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        # learnable_pos= True,\n",
    "    )\n",
    "\n",
    "    large_default_dict =dict(\n",
    "        drop_path_rate=0, use_abs_pos_emb=True,  # as in table 11\n",
    "        patch_size=patch_size,patch_stride=patch_stride,in_chans=in_chans, out_chans=out_chans, embed_dim=1024, depth=24,\n",
    "        num_heads=16, mlp_ratio=4, qkv_bias=True, \n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        # learnable_pos= True,\n",
    "    )\n",
    "\n",
    "\n",
    "    huge_default_dict =dict(\n",
    "        drop_path_rate=0, use_abs_pos_emb=True,  # as in table 11\n",
    "        patch_size=patch_size,patch_stride=patch_stride,in_chans=in_chans, out_chans=out_chans, embed_dim=2048, depth=24,\n",
    "        num_heads=16, mlp_ratio=4, qkv_bias=True, \n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        # learnable_pos= True,\n",
    "    )\n",
    "\n",
    "    if arch == \"vit_base\":\n",
    "        recursive_update(base_default_dict, kwargs)\n",
    "        encoder = ViT_Decoder(**base_default_dict)\n",
    "\n",
    "    elif arch == \"vit_large\":\n",
    "\n",
    "        recursive_update(large_default_dict, kwargs)\n",
    "        encoder = ViT_Decoder(**large_default_dict)\n",
    "\n",
    "    elif arch == \"vit_huge\":\n",
    "        recursive_update(huge_default_dict, kwargs)\n",
    "        encoder = ViT_Decoder(**huge_default_dict)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Architecture undefined!\")\n",
    "\n",
    "\n",
    "\n",
    "    if finetune_model is not None:\n",
    "        import io\n",
    "        pretrained_dict = torch.load(finetune_model, map_location='cpu')['state_dict']\n",
    "\n",
    "        model_dict = state_dict()\n",
    "        pretrained_dict_filter ={}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k[9:] in model_dict.keys():\n",
    "                pretrained_dict_filter.update({k[9:]: v})\n",
    "        load_state_dict(encoder, pretrained_dict_filter, strict=False, logger=dummy_logger)\n",
    "\n",
    "\n",
    "        print(\n",
    "            \"Missing keys: {}\".format(list(set(model_dict) - set(pretrained_dict_filter)\n",
    "                                           )))\n",
    "        model_dict.update(pretrained_dict_filter)\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        load_state_dict(model_dict)\n",
    "        del pretrained_dict\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c30c74b-f038-4e3f-920b-2e5e7f39f0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA\n",
      "MHA\n",
      "MHA\n",
      "MHA\n",
      "MHA\n",
      "MHA\n",
      "MHA\n"
     ]
    }
   ],
   "source": [
    "ddconfig=dict(\n",
    "                arch = 'vit_large',\n",
    "                pretrained_model = '',\n",
    "                patch_size=(12,12),\n",
    "                patch_stride=(12,12),\n",
    "                in_chans=69,\n",
    "                out_chans=69,\n",
    "                kwargs=dict( #will update the default vit config\n",
    "                    z_dim =  69,\n",
    "                    depth = 24,\n",
    "                    embed_dim=1024,\n",
    "                    num_heads=16,\n",
    "                    learnable_pos= True,\n",
    "                    window= True,\n",
    "                    window_size = [(24, 24), (12, 48), (48, 12)],\n",
    "                    interval = 4,\n",
    "                    drop_path_rate= 0.,\n",
    "                    round_padding= True,\n",
    "                    pad_attn_mask= True , # to_do: ablation\n",
    "                    test_pos_mode= 'learnable_simple_interpolate', # to_do: ablation\n",
    "                    lms_checkpoint_train= True,\n",
    "                    img_size= (721, 1440)\n",
    "                ))\n",
    "\n",
    "import yaml\n",
    "def load_and_convert_config(yaml_path):\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    '''\n",
    "    params = config.get('model', {}).get('params', {})\n",
    "    # 将列表转换为元组以确保与原配置兼容\n",
    "    config[\"model\"]['patch_size'] = tuple(params['patch_size'])\n",
    "    config[\"model\"]['patch_stride'] = tuple(params['patch_stride'])\n",
    "    # 处理kwargs中的嵌套结构\n",
    "    if 'kwargs' in config[\"model\"]:\n",
    "        kwargs = config[\"model\"]['kwargs']\n",
    "        kwargs['window_size'] = [tuple(ws) for ws in kwargs.get('window_size', [])]\n",
    "        kwargs['img_size'] = tuple(kwargs.get('img_size', []))\n",
    "    return params\n",
    "    '''\n",
    "    return config\n",
    "# 加载配置\n",
    "#config_params = load_and_convert_config('config.yaml')\n",
    "#params = config_params.get('model', {}).get('params', {})\n",
    "# 实例化Encoder\n",
    "#model = Encoder(**params)\n",
    "\n",
    "model1  = Encoder(**ddconfig)\n",
    "model2 = Decoder(**ddconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5dfa682-e4d7-4e09-b5d0-c46aa2ef2723",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m input_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m69\u001b[39m,\u001b[38;5;241m721\u001b[39m,\u001b[38;5;241m1440\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#input_ = F.pad(input_, (48, 48, 24, 23), \"constant\", 0)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_ = torch.rand([1,69,721,1440]).to(device)\n",
    "#input_ = F.pad(input_, (48, 48, 24, 23), \"constant\", 0)\n",
    "model(input_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abdbf0-5b10-4bfe-b4ea-bb9c3d0f703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8367c73-a174-4b7a-a7c5-1bf32c2a192e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
