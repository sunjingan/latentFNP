seed: 0
cuda: 0
world_size: 1
per_cpus: 4
max_epoch: 1000
batch_size: 4
lead_time: 24
ratio: 0.9
resolution: 128
init_method: tcp://127.0.0.1:19111
rundir: ./configs/VAE/mask0.9_lead24h_res128
rank: 0
local_rank: 0
distributed: false
gpu: 0
cfg: ./configs/VAE/training_options.yaml
cfg_params:
  vnames: &id001
    single_level_vnames:
    - u10
    - v10
    - t2m
    - msl
    multi_level_vnames:
    - z
    - q
    - u
    - v
    - t
    hight_level_list:
    - 50
    - 100
    - 150
    - 200
    - 250
    - 300
    - 400
    - 500
    - 600
    - 700
    - 850
    - 925
    - 1000
  dataset:
    train:
      type: era5_npy_f32
      data_dir: s3://era5_np_float32
      train_stride: 6
      file_stride: 6
      sample_stride: 1
      vnames: *id001
      length: 6
    valid:
      type: era5_npy_f32
      data_dir: s3://era5_np_float32
      train_stride: 6
      file_stride: 6
      sample_stride: 1
      vnames: *id001
      length: 6
    test:
      type: era5_npy_f32
      data_dir: s3://era5_np_float32
      train_stride: 6
      file_stride: 6
      sample_stride: 1
      vnames: *id001
  dataloader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true
  model:
    type: VAE
    VAEparams:
      arch: vit_large
      pretrained_model: ''
      patch_size:
      - 4
      - 4
      patch_stride:
      - 4
      - 4
      in_chans: 69
      out_chans: 69
      kwargs:
        z_dim: 69
        learnable_pos: true
        window: true
        window_size:
        - - 24
          - 24
        - - 12
          - 48
        - - 48
          - 12
        interval: 4
        drop_path_rate: 0.0
        round_padding: true
        pad_attn_mask: true
        test_pos_mode: learnable_simple_interpolate
        lms_checkpoint_train: true
        img_size:
        - 128
        - 256
    FNPparams:
      n_channels:
      - 16
      - 48
      - 48
      - 48
      - 48
      - 48
      r_dim: 128
      use_nfl: true
      use_dam: true
    criterion: CNPFLoss
    optimizer:
      type: AdamW
      params:
        lr: 0.0001
        betas:
        - 0.9
        - 0.9
        weight_decay: 0.01
    lr_scheduler:
      type: OneCycleLR
      params:
        max_lr: 0.0001
        pct_start: 0.1
        anneal_strategy: cos
        div_factor: 100
        final_div_factor: 1000
